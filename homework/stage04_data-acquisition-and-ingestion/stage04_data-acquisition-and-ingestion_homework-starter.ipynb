{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 04: Data Acquisition and Ingestion\n",
    "Name: \n",
    "Date: \n",
    "\n",
    "## Objectives\n",
    "- API ingestion with secrets in `.env`\n",
    "- Scrape a permitted public table\n",
    "- Validate and save raw data to `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPHAVANTAGE_API_KEY loaded? True\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "RAW = pathlib.Path('data/raw'); RAW.mkdir(parents=True, exist_ok=True)\n",
    "load_dotenv(); print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers (use or modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved', path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — API Pull (Required)\n",
    "Choose an endpoint (e.g., Alpha Vantage or use `yfinance` fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 columns: ['index', '1. open', '2. high', '3. low', '4. close', '5. volume']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'missing': [], 'shape': (100, 2), 'na_total': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYMBOL = 'AAPL'\n",
    "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "if USE_ALPHA:\n",
    "    url = 'https://www.alphavantage.co/query'\n",
    "    #adjusted doesn't seem to work, premium endpoint instead of free;\n",
    "    # params = {'function':'TIME_SERIES_DAILY_ADJUSTED','symbol':SYMBOL,'outputsize':'compact','apikey':os.getenv('ALPHAVANTAGE_API_KEY')}\n",
    "    params = {'function':'TIME_SERIES_DAILY','symbol':SYMBOL,'outputsize':'compact','apikey':os.getenv('ALPHAVANTAGE_API_KEY')}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    key = [k for k in js if 'Time Series' in k][0]\n",
    "    df1 = pd.DataFrame(js[key]).T.reset_index()\n",
    "    print('df1 columns:', df1.columns.tolist())\n",
    "    #less columns in the free endpoint, had to remove some\n",
    "    #df_api = pd.DataFrame(js[key]).T.reset_index().rename(columns={'index':'date','5. adjusted close':'adj_close'})[['date','adj_close']]\n",
    "    df_api = pd.DataFrame(js[key]).T.reset_index().rename(columns={'index':'date',\"4. close\":\"close\"})[['date','close']]\n",
    "    df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "    #df_api['adj_close'] = pd.to_numeric(df_api['adj_close'])\n",
    "    #replace with free verison\n",
    "    df_api['close'] = pd.to_numeric(df_api['close'])\n",
    "else:\n",
    "    import yfinance as yf\n",
    "    df_api = yf.download(SYMBOL, period='3mo', interval='1d').reset_index()[['Date','Adj Close']]\n",
    "    df_api.columns = ['date','adj_close']\n",
    "#adjust for close\n",
    "# v_api = validate(df_api, ['date','adj_close']); v_api\n",
    "v_api = validate(df_api, ['date','close']); v_api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/raw/api_source-alpha_symbol-AAPL_20250820-103003.csv\n"
     ]
    }
   ],
   "source": [
    "_ = save_csv(df_api.sort_values('date'), prefix='api', source='alpha' if USE_ALPHA else 'yfinance', symbol=SYMBOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — Scrape a Public Table (Required)\n",
    "Replace `SCRAPE_URL` with a permitted page containing a simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NEGG', 'Newegg Commerce, Inc.', '', '101.41+12.01(+13.47%)', '+12.01', '+13.47%', '467,170', '1.136M', '1.975B', '--', '+419.77%', ''], ['VKTX', 'Viking Therapeutics, Inc.', '', '25.94+1.58(+6.49%)', '+1.58', '+6.49%', '9.135M', '4.857M', '2.917B', '--', '-62.38%', ''], ['HTHT', 'H World Group Limited', '', '35.17+1.82(+5.44%)', '+1.82', '+5.44%', '1.407M', '1.763M', '10.794B', '24.59', '+16.94%', ''], ['COCO', 'The Vita Coco Company, Inc.', '', '35.83+1.89(+5.57%)', '+1.89', '+5.57%', '345,220', '645,720', '2.036B', '33.49', '+33.89%', ''], ['TJX', 'The TJX Companies, Inc.', '', '140.76+6.14(+4.56%)', '+6.14', '+4.56%', '3.953M', '5.24M', '157.029B', '33.12', '+11.97%', ''], ['MDT', 'Medtronic plc', '', '93.56+3.65(+4.07%)', '+3.65', '+4.07%', '3M', '8.067M', '119.868B', '25.92', '+2.70%', ''], ['PPC', \"Pilgrim's Pride Corporation\", '', '47.46+1.77(+3.86%)', '+1.77', '+3.86%', '345,299', '1.205M', '11.273B', '9.14', '+8.69%', ''], ['AS', 'Amer Sports, Inc.', '', '36.84+1.10(+3.08%)', '+1.10', '+3.08%', '1.995M', '4.176M', '20.43B', '99.57', '+152.58%', ''], ['NGD', 'New Gold Inc.', '', '5.30+0.16(+3.22%)', '+0.16', '+3.22%', '3.468M', '19.775M', '4.209B', '26.48', '+90.71%', ''], ['UA', 'Under Armour, Inc.', '', '5.11+0.11(+2.20%)', '+0.11', '+2.20%', '1.377M', '3.724M', '2.21B', '23.23', '-38.27%', '']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'missing': [], 'shape': (10, 12), 'na_total': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SCRAPE_URL = 'https://example.com/markets-table'  # TODO: replace with permitted page\n",
    "SCRAPE_URL = 'https://finance.yahoo.com/gainers'  # TODO: replace with permitted page\n",
    "#good website for gaining companies by yahoo which is one of the originals\n",
    "headers = {'User-Agent':'AFE-Homework/1.0'}\n",
    "try:\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30); resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "    header, *data = [r for r in rows if r]\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "    print(data)\n",
    "except Exception as e:\n",
    "    print('Scrape failed, using inline demo table:', e)\n",
    "    html = '<table><tr><th>Ticker</th><th>Price</th></tr><tr><td>AAA</td><td>101.2</td></tr></table>'\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "    header, *data = [r for r in rows if r]\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "if 'Price' in df_scrape.columns:\n",
    "    df_scrape['Price'] = pd.to_numeric(df_scrape['Price'], errors='coerce')\n",
    "v_scrape = validate(df_scrape, list(df_scrape.columns)); v_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/raw/scrape_site-yahoo_finance_table-gainers_20250820-103004.csv\n"
     ]
    }
   ],
   "source": [
    "_ = save_csv(df_scrape, prefix='scrape', site='yahoo_finance', table='gainers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "- API Source: https://www.alphavantage.co/query with params TIME_SERIES_DAILY for funciotn type, AAPL for symbol, compact outputsize, and my free tier api key\n",
    "- Scrape Source: https://finance.yahoo.com/gainers -> table of top gainers in the market\n",
    "- Assumptions & risks: To the API there are definitely rate limits, but in terms of the scrape since its just extracting from the http request that can be done whenever. Selector fragility was seen when the endpoint user here was changed to a premium endpoint. Therefore the coed here can only be guaranteed to work for AlphaVantage for only some time. Schema changes can be more likely than selector fragility due to endpoint changes, but they are still unlikely considering I only use a handful of columns\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fe-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
